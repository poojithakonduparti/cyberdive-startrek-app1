# Startrek-Flyhigh: Admin Portal

![Webinar-pana](https://user-images.githubusercontent.com/46862684/229015820-c303a49e-dd60-4381-a77a-165e0f9aa562.svg)

----- 

> âœ… Active status <br>


----- 

## Index
  - [Objective ðŸŽ¯](#objective)
  - [Abstract ðŸ“](#abstract)
  - [Architecture Diagram ðŸ—](#architecture-diagram)
  - [Project Components ðŸ’½](#project-components)
    - [AWS AppSync GraphQL API](#AppSync)
    - [AWS DynamoDB ](#DynamoDB)
    - [AWS Cognito](#Cognito)
    - [AWS Amplify](#Amplify)
    - [React](#React)


  - [How to run the application ðŸ’»](#how-to-run-the-application-locally)
----- 

## Objective
Build an Employee Information Portal using Amplify and React where the authenticated administration of Flyhigh can easily access the transferred employees information from [DynamoDB](https://aws.amazon.com/dynamodb/#:~:text=Amazon%20DynamoDB%20is%20a%20fully,data%20import%20and%20export%20tools.) This app should be able to provide access to deleting and updating the employee details using [AppSync]


## Abstract
The task involves building a decoupled architecture for the Flyhigh-Startrek Admin Portal:

- Login Page to the Admin Portal using authorized credentials
- View employee ID and Name in a table on the Home Page
- Click on an employee to open a side panel to edit and delete the employee information
- Auto Update the Employee Table in **real time** on the Admin Portal when a new employee record is added 


## Architecture Diagram
![Architecture](https://github.com/BigDataIA-Spring2023-Team-08/assignment04-meeting-intelligence-tool/blob/main/architecture%20diagram/whisper_and_chat_api_architecture.png?raw=true)


## Project Components

### APIs
**Whisper API:** API for [Whisper](https://openai.com/research/whisper) speech-to-text open-source model which provides two endpoints for transcriptions and translations and accepts variety of formats (m4a, mp3, mp4, mpeg, mpga, wav, webm). For the purpose of this assignment, Whisper API has been implemented to transcribe audio from test meeting recordings and user uploaded audio files in mp3 format.

**GPT 3.5 API:** API for [ChatGPT 3.5](https://openai.com/research/whisper) model which takes sequence of messages coupled with metadata as tokens to generate text completion which can either be natural language or code. For the purpose of this assignment, GPT 3.5 API has been implemented to build a query engine complemented by the transcripts generated by Whisper as well as generate transcript questionnaire.

### Streamlit
Python library [Streamlit](https://streamlit.iohttps://streamlit.io) has been implemented in this application for its user interface. Streamlit offers user friendly experience to assist users in :

>  Upload audio files to S3 bucket 

>  Select meetings from a list

>  Generate general questionnaire from the selected meeting

>  Ask questions related to meeting using the query engine

### Airflow
Airflow is an open-source platform for data orchestration, through which data workflows can be scheduled, monitored and managed. In this application, Airflow is integrated to automate the workflow of the application with the help of following DAG's (Directed Acyclic Graphs):

1) **Adhoc Dags** - Dags which can be triggered using REST API calls.

> `Task 1`: Audio files of meeting uploaded by users are stored in `Adhoc` folder inside S3 bucket which is read by the Dag.

> `Task 2`: Audio file is sent to Whisper AI to generate transcriptions

> `Task 3`: Transcripts are written in `Processed` folder inside S3 bucket

> `Task 4`: ChatGPT API is called for querying questions

2) **Batch Dags** - Dags which are scheduled using cron.

> `Task 1`: Read audio files stored in `Batch` folder inside S3 bucket.

> `Task 2`: Audio file is sent to Whisper AI to generate transcriptions

> `Task 3`: Transcripts are written in `Processed` folder inside S3 bucket

> `Task 4`: ChatGPT API is called for querying questions



## How to run the application locally

1. Clone the repo to get all the source code on your machine

2. Within the [airflow folder](https://github.com/BigDataIA-Spring2023-Team-08/assignment04-meeting-intelligence-tool/tree/main/airflow), create a `.env` file with just the following line: 

        AIRFLOW_UID=1001
        
Note: no need to add your credentials in this .env file since the credentials for the airflow app are to be added as said in the next point

3. Edit lines 66-71 in the [`docker-compose.yml`](https://github.com/BigDataIA-Spring2023-Team-08/assignment04-meeting-intelligence-tool/blob/main/airflow/docker-compose.yaml) found within the airflow folder to add your API keys

4. Once done, create a virtual environment and install all requirements from the [`requirements.txt`](https://github.com/BigDataIA-Spring2023-Team-08/assignment04-meeting-intelligence-tool/blob/main/airflow/requirements.txt) file present

5. Finally, execute following line to get airflow running: 

        docker compose up

Lets us get the streamlit frontend running now:

6. Within the [streamlit-app folder](https://github.com/BigDataIA-Spring2023-Team-08/assignment04-meeting-intelligence-tool/tree/main/streamlit-app), create a `.env` file with following variables and your key: 

        ACCESS_KEY=yourkey
        AWS_SECRET_KEY=yourkey
        USER_BUCKET_NAME=meeting-intelligence-tool
        OPENAI_KEY=yourkey
        AIRFLOW_URL=http://34.73.90.193:8081/

Note: airflow URL should be the URL you just got after doing docker compose up

7. Once done, create a virtual environment and install all requirements from the [`requirements.txt`](https://github.com/BigDataIA-Spring2023-Team-08/assignment04-meeting-intelligence-tool/blob/main/streamlit-app/requirements.txt) file present 

8. Finally, execute following line to get streamlit running: 

        docker compose up

9. Access application through the port you just opened by running docker compose up for Streamlit
